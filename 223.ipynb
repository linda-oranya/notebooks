{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "223.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaXVR7K7rLJK"
      },
      "source": [
        "# Module 2: Data Engineering\n",
        "## Sprint 2: SQL and Data Scraping\n",
        "## Subproject 3: Web scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkx20J8rrLJN"
      },
      "source": [
        "During this lesson, you will learn all that you need to know to start scraping the internet. You will get familiar with the structure of websites, key elements of HTML. You will be introduced to the `requests` and `bs4` libraries that combined enable robust web scraping workflow that can be used to create datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKeAe46zrLJN"
      },
      "source": [
        "## Learning outcomes\n",
        "- At the end of this lesson you will be able to create web page's scraping strategy\n",
        "- You will be able to understand HTML document's structure\n",
        "- You will know how to extract specific information form websites\n",
        "- You will know how to scrape websites to create your own datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPrpScPZrLJO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDzF7eV9rLJO"
      },
      "source": [
        "## Why we scrape the internet?\n",
        "Data science without data is not much of a science. In most cases, you can find many great curated clean datasets online (for example Kaggle). Sometimes you need specific data for your project to succeed. How can you collect one? Well, you can always search the web for particular information and manually create your own dataset. Unfortunately, these days many models require quite large data samples and it would be time-consuming and sometimes even impossible to manually collect +10k individual samples to your dataset. Luckily for us, there is one great method that enables us to automate this process. It is called data scraping. This technique is widely used by software agents for different purposes. One of the examples is Google. The software giant goes to every page, scans it, and puts the information into its databases for later indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlnp1Zx_rLJP"
      },
      "source": [
        "## What is a Webpage?\n",
        "Almost all web pages are text documents presented in `.html` format. HTML is a markup language similar to Markdown that you use to write README files on GitHub. To be able to scrape information from HTML files, first, you need to understand how to create one. You have to watch [this](https://www.youtube.com/watch?v=pQN-pnXPaVg) freecodecamp video that explains the main concepts of creating **HTML based** website. Later, we will take a closer look into individual subjects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROeZQBL3rLJP"
      },
      "source": [
        "## Anatomy of an HTML element\n",
        "### Nesting elements\n",
        "Every HTML element lives inside other HTML element. HTML documents have their own strict structure. Here is a base skeleton of every HTML document:\n",
        "```html\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "    <head>\n",
        "        <title>Page Title</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>My First Heading</h1>\n",
        "        <p>My first paragraph.</p>\n",
        "    </body>\n",
        "</html> \n",
        "```\n",
        "As you can see there are three main parts of the HTML document:\n",
        "* `<html>` - place where all html content rests in\n",
        "* `<head>` - place where meta information of page is placed: title, scripts, stylesheets, etc.\n",
        "* `<body>` - the main part of the document: all information is placed here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nODQXM7srLJP"
      },
      "source": [
        "### Tags, Classes and Ids\n",
        "As you will primarily be working with getting information from HTML documents, let's talk about what you can find inside `<body>` part. All information inside HTML documents is presented via [html tags](https://developer.mozilla.org/en-US/docs/Web/HTML/Element). There is a large number of HTML tags available but almost all of them share the same properties: they are place of some block inside the document and they format presented information. For example:\n",
        "```html\n",
        "<div class=\"content\">\n",
        "    <h1>Text</h1>\n",
        "    <h2 id=\"subtxt\">Subtext</h2>\n",
        "</div>\n",
        "```\n",
        "As you can see, elements are nested inside other elements. Information is presented inside HTML tags. HTML elements can be differentiated using **class** and **id**. Classes are used to group certain elements and apply scripts and styles to them. The same is with ids but the idea is that ids should be unique per document but sadly this is not always the case. Why tags and classes are important to us? We can use them to select parts of the HTML element that we want to extract information from. Let's say there is a page where capitals and native language of EU countries are listed:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0f4-RU7rLJQ"
      },
      "source": [
        "```html\n",
        "<div>\n",
        "    <ul class=\"capitals\">\n",
        "        <li>Vilnius</li>\n",
        "        <li>Paris</li>\n",
        "        <li>Riga</li>\n",
        "        <li>Tallinn</li>\n",
        "    </ul>\n",
        "    <ul class=\"languages\">\n",
        "        <li>Lithuanian</li>\n",
        "        <li>French</li>\n",
        "        <li>Latvian</li>\n",
        "        <li>Estonian</li>\n",
        "    </ul>\n",
        "</div>\n",
        "```\n",
        "If you just select `li` as the tag you want to extract information from you will get all: capitals and languages but you want only capitals. `ul` selection also would not give an expected result as there are two `ul` lists in the document. What you want to do is to select `.capitals ul` - by providing class we are able to select distinct `ul` and extract wanted information.\n",
        "\n",
        "[Here](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML) is a full introduction to HTML made Mozilla. You can complete all of it if you want to get deep knowledge in the subject. If you want just do some web scraping, knowledge of the HTML file structure and its elements is more than enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx6TyG11rLJR"
      },
      "source": [
        "## Web Scraping\n",
        "As mentioned in the beginning of the lesson, web scraping is used to automate information collection from websites and to create datasets for later usage. There are many great tools that can be used to scrape websites. Most popular ones are:\n",
        "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "* [Scrapy](https://scrapy.org/)\n",
        "* [Selenium](https://www.selenium.dev/)\n",
        "\n",
        "Scrapy is a more advanced tool that need a bit more setup, Selenium is really similar to Beautiful Soup but Beautiful Soup is more popular and easier to use option, so in this lesson we will be scraping websites using this tool.\n",
        "First, you should watch [this freecodecamp video](https://www.youtube.com/watch?v=87Gx3U0BDlo) about Soup that covers all basic usages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nTwKdMnrLJR"
      },
      "source": [
        "## Setup\n",
        "Setup of Soup is simple and does not require any difficult steps to be made. Just use `pip` to install it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFHHw1fLrLJS",
        "outputId": "0d986f76-daf4-4049-8229-36db86f7d5b3"
      },
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J-nC63VrLJT"
      },
      "source": [
        "Now we can use it to extract information from HTML elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJNuXGS1rLJT",
        "outputId": "a53196b8-c67d-4e23-c1a4-5fe451237d01"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Defining some HTML format text\n",
        "html_doc = \"\"\"\n",
        "<html>\n",
        "<head><title>Example text</title></head>\n",
        "<body><p class=\"text\">Text to extract</p></body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Creating soup object using BeutifulSoup\n",
        "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
        "\n",
        "# Finding all elements inside HTML that have p tag and text class\n",
        "p_results = soup.find_all(\"p\", class_=\"text\")\n",
        "\n",
        "# Extracting text value from found elements\n",
        "for p_result in p_results:\n",
        "    print(p_result.text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to extract\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22lQawO2rLJV"
      },
      "source": [
        "As you can see from the example, we successfully found desired elements in the HTML document and extracted text values from them. This is pretty neat but the power of web scraping comes from automation and real-world examples. So now let's talk about how to extract information from real web pages. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-pgnzfIrLJV"
      },
      "source": [
        "## Downloading pages\n",
        "To get HTML format document to extract data from you need to download it. It is not that difficult if you are using Python `requests`. We will use a popular forum [Hacker News](https://news.ycombinator.com/) as a website that we will scrape information from:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIoGA_AQrLJW",
        "outputId": "e72b55ba-7a39-4803-b199-56d8552c27f6"
      },
      "source": [
        "import requests\n",
        "\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "page = requests.get(URL)\n",
        "\n",
        "page"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKPJX2vArLJW"
      },
      "source": [
        "Some websites can be protected from web scraping. In this case, you need to find ways of exploiting their systems. Most of the time addition of `User-Agent` header in the request should do the thing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QgafgPrrLJW",
        "outputId": "12d2bddc-d0e8-4802-b860-d82ecd89ba30"
      },
      "source": [
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "page"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEkZmC4xrLJX"
      },
      "source": [
        "## Looking for text\n",
        "Now that we have a page downloaded we can start extracting information from it. First, we need to find exact parts of the page we want to get text from. We can do it by going to the web page and using [inspect element](https://zapier.com/blog/inspect-element-tutorial/) functionality of your browser. Let's say we want to collect titles of all posts in the page:\n",
        "<div><img src=\"https://i.imgur.com/HJ8DwAN.png\" width=\"600px\"/></div>\n",
        "We can see that information that we need is inside the `<a>` tag with the `storylink` class. We can use `soup` to extract the information by providing newly learned properties of the elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBNtMF0SrLJX",
        "outputId": "b67531f6-8162-4835-89fc-b339bd0c334e"
      },
      "source": [
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "for title in soup.find_all(\"a\", class_=\"storylink\"):\n",
        "    print(title.text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dividend Cripples Saudi Aramco\n",
            "macOS Nix setup: an alternative to Homebrew (2020)\n",
            "Carlos Ghosn: How I escaped Japan in a box\n",
            "RabbitMQ Streams Overview\n",
            "If you want to transform IT, start with finance\n",
            "Beating TimSort at Merging\n",
            "The worst volume control UI in the world\n",
            "Why is standing more tiring than walking? [audio]\n",
            "Give me /events, not webhooks\n",
            "Plane Food Simulator 2021\n",
            "Mozart Data (YC S20) Is Hiring a UI/UX Designer\n",
            "Reducing the Computational Cost of Deep Reinforcement Learning Research\n",
            "EXIF-based intrinsic image sizing explainer\n",
            "Scalable but Wasteful, or why fast replication protocols are slow\n",
            "Jelly\n",
            "$100M xPrize for Carbon Removal\n",
            "AWS Cost Saving Recommendations\n",
            "Inventor harvests methane gas from ditches and ponds to power his moped\n",
            "MacKichan Software, maker of Scientific Word, has gone out of business\n",
            "Emacs Tramp over AWS SSM APIs\n",
            "Firefox 90\n",
            "Night of the Guillotine: The Fall of Robespierre\n",
            "A privacy war is raging inside the W3C\n",
            "Show HN: maildog - Hosting email forwarding service on AWS with GitHub Actions\n",
            "Good and Bad Monitoring\n",
            "Apache Heron: A realtime, distributed, fault-tolerant stream processing engine\n",
            "Building a vision of life without work (2015)\n",
            "Launch HN: Café (YC S21) – Find the best days to go to the office\n",
            "A review and how-to guide for Microsoft Form Recognizer\n",
            "Downgrade User Agent Client Hints to 'harmful'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETIlvTcqrLJX"
      },
      "source": [
        "## Storing collected data\n",
        "You can use collected data to create pandas DataFrames. This enables you to make various data processing operations (removing corrupted information, filling in missing data). Pandas will also make your life easier when you will need to export collected information to `csv` files or insert data to databases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeOvXg4WrLJX",
        "outputId": "f30d016d-9378-4fe2-bb4f-c3f44277490d"
      },
      "source": [
        "# Run this if needed\n",
        "!pip install pandas"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "jtSTINFHrLJX",
        "outputId": "0466e707-4b3f-4851-9874-c3839c51728e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "page = requests.get(\"https://news.ycombinator.com/\", headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "collected_information = [{\"title\": title.text} for title in soup.find_all(\"a\", class_=\"storylink\")]\n",
        "\n",
        "df = pd.DataFrame(collected_information)\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dividend Cripples Saudi Aramco</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>macOS Nix setup: an alternative to Homebrew (2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Carlos Ghosn: How I escaped Japan in a box</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RabbitMQ Streams Overview</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you want to transform IT, start with finance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title\n",
              "0                     Dividend Cripples Saudi Aramco\n",
              "1  macOS Nix setup: an alternative to Homebrew (2...\n",
              "2         Carlos Ghosn: How I escaped Japan in a box\n",
              "3                          RabbitMQ Streams Overview\n",
              "4    If you want to transform IT, start with finance"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJuoKGIwrLJY"
      },
      "source": [
        "You can see how powerful `soup` is for doing web scraping tasks! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEVbSGUfrLJY"
      },
      "source": [
        "## Exercise\n",
        "Now it is your time to shine: you will need to extract more information from the web page and put it inside pandas DataFrame. Write code inside the cells for tests below to pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Mci3yvrLJY"
      },
      "source": [
        "# Get score of every element\n",
        "collected_scores = [{\"score\": score.text} for score in soup.find_all(\"span\", class_=\"score\")]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPTe5kZijPgS",
        "outputId": "c0efcf48-689c-4a0f-cebf-3b573312069f"
      },
      "source": [
        "collected_scores"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': '57 points'},\n",
              " {'score': '226 points'},\n",
              " {'score': '66 points'},\n",
              " {'score': '104 points'},\n",
              " {'score': '109 points'},\n",
              " {'score': '192 points'},\n",
              " {'score': '12 points'},\n",
              " {'score': '71 points'},\n",
              " {'score': '277 points'},\n",
              " {'score': '51 points'},\n",
              " {'score': '57 points'},\n",
              " {'score': '8 points'},\n",
              " {'score': '42 points'},\n",
              " {'score': '28 points'},\n",
              " {'score': '299 points'},\n",
              " {'score': '102 points'},\n",
              " {'score': '236 points'},\n",
              " {'score': '202 points'},\n",
              " {'score': '91 points'},\n",
              " {'score': '369 points'},\n",
              " {'score': '45 points'},\n",
              " {'score': '113 points'},\n",
              " {'score': '118 points'},\n",
              " {'score': '71 points'},\n",
              " {'score': '146 points'},\n",
              " {'score': '359 points'},\n",
              " {'score': '67 points'},\n",
              " {'score': '23 points'},\n",
              " {'score': '133 points'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCMleCY4rLJZ"
      },
      "source": [
        "assert type(collected_scores[0]) == dict\n",
        "assert list(collected_scores[0].keys())[0] == \"score\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDo6k_ATrLJZ"
      },
      "source": [
        "# Get age of the post\n",
        "collected_age = [{\"post_age\":age.text} for age in soup.find_all('span',attrs={'class':'age'})]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5KCKISQj2W4",
        "outputId": "7ab54df2-e51c-4bd2-d133-d177174cb307"
      },
      "source": [
        "collected_age"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'post_age': '54 minutes ago'},\n",
              " {'post_age': '4 hours ago'},\n",
              " {'post_age': '2 hours ago'},\n",
              " {'post_age': '3 hours ago'},\n",
              " {'post_age': '4 hours ago'},\n",
              " {'post_age': '6 hours ago'},\n",
              " {'post_age': '1 hour ago'},\n",
              " {'post_age': '4 hours ago'},\n",
              " {'post_age': '7 hours ago'},\n",
              " {'post_age': '4 hours ago'},\n",
              " {'post_age': '2 hours ago'},\n",
              " {'post_age': '5 hours ago'},\n",
              " {'post_age': '1 hour ago'},\n",
              " {'post_age': '4 hours ago'},\n",
              " {'post_age': '2 hours ago'},\n",
              " {'post_age': '7 hours ago'},\n",
              " {'post_age': '6 hours ago'},\n",
              " {'post_age': '14 hours ago'},\n",
              " {'post_age': '12 hours ago'},\n",
              " {'post_age': '9 hours ago'},\n",
              " {'post_age': '9 hours ago'},\n",
              " {'post_age': '6 hours ago'},\n",
              " {'post_age': '3 hours ago'},\n",
              " {'post_age': '11 hours ago'},\n",
              " {'post_age': '7 hours ago'},\n",
              " {'post_age': '12 hours ago'},\n",
              " {'post_age': '9 hours ago'},\n",
              " {'post_age': '7 hours ago'},\n",
              " {'post_age': '4 hours ago'},\n",
              " {'post_age': '13 hours ago'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysMIewUXrLJZ"
      },
      "source": [
        "assert type(collected_age[0]) == dict\n",
        "assert list(collected_age[0].keys())[0] == \"post_age\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2kI5NRBrLJZ"
      },
      "source": [
        "# Get post's author.\n",
        "collected_users = [{\"post_author\":user.text} for user in soup.find_all('a',attrs={'class':'hnuser'})]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-3lsCxkkuRU",
        "outputId": "50dee0c8-dcb7-49b7-dd9d-2cc8f689a9ce"
      },
      "source": [
        "collected_users[:9]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'post_author': 'syncsynchalt'},\n",
              " {'post_author': 'notamy'},\n",
              " {'post_author': 'phpnode'},\n",
              " {'post_author': 'DeadTrickster'},\n",
              " {'post_author': 'feross'},\n",
              " {'post_author': 'andrewstetsenko'},\n",
              " {'post_author': 'yankcrime'},\n",
              " {'post_author': 'open-source-ux'},\n",
              " {'post_author': 'todsacerdoti'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so7J5bK5rLJZ"
      },
      "source": [
        "assert type(collected_users[0]) == dict\n",
        "assert list(collected_users[0].keys())[0] == \"post_author\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkfBpVH1rLJZ"
      },
      "source": [
        "Now that you are able to extract information from the page, you need to automate this process and extract information from multiple pages at once and add the information to one `dict` (appending it to list). You will need to complete a function that returns a dataframe with columns: title, link, and age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYYn91PLlUAM"
      },
      "source": [
        "import time\n",
        "def scrape_page(page):\n",
        "  titles = []\n",
        "  scores =[]\n",
        "  ages = []\n",
        "  users = []\n",
        "  page_number = 3\n",
        "  for n in range(1,page_number):\n",
        "    time.sleep(30)\n",
        "    data_info = {}\n",
        "    page = requests.get(f\"https://news.ycombinator.com/news?p={n}\", headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    collected_information = [{\"title\": title.text} for title in soup.find_all(\"a\", class_=\"storylink\")]\n",
        "    collected_scores = [{\"score\": score.text} for score in soup.find_all(\"span\", class_=\"score\")]\n",
        "    collected_age = [{\"post_age\":age.text} for age in soup.find_all('span',attrs={'class':'age'})]\n",
        "    collected_users = [{\"post_author\":user.text} for user in soup.find_all('a',attrs={'class':'hnuser'})]\n",
        "\n",
        "    for item in collected_information:\n",
        "      try:\n",
        "        titles.append(item['title'].replace(\"“\",'').replace(\"”\",''))\n",
        "      except:\n",
        "        titles.append(None)\n",
        "    for item in collected_scores:\n",
        "      try:\n",
        "       scores.append(item['score'])\n",
        "      except:\n",
        "        scores.append(None)\n",
        "    for item in collected_age:\n",
        "      try:\n",
        "       ages.append(item['post_age'])\n",
        "      except:\n",
        "        ages.append(None)\n",
        "    for item in collected_users:\n",
        "      try:\n",
        "        users.append(item['post_author'])\n",
        "      except:\n",
        "        user.append(None)\n",
        "  print(len(users),len(scores),len(titles),len(ages))\n",
        "  print(scores)\n",
        "  print(titles)\n",
        "  dataframe = pd.DataFrame({\n",
        "      \"title\": titles,\n",
        "      #\"scores\":scores,\n",
        "      \"post_age\":ages,\n",
        "      #\"post_authors\":users\n",
        "  })\n",
        "  return dataframe"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbT6o3yu9yyM"
      },
      "source": [
        "# string = \"Apple's “iCloud Private Relay” broke risk based authentication\"\n",
        "# new_string = string.replace(\"'\", \"\").replace(\"“\",'').replace(\"”\",'')\n",
        "\n",
        "# print(new_string)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo8H1H5InmFa",
        "outputId": "7c912306-4fda-485e-98ef-e230c875d2e6"
      },
      "source": [
        "data=scrape_page('https://news.ycombinator.com/news')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59 59 60 60\n",
            "['59 points', '227 points', '67 points', '105 points', '109 points', '192 points', '14 points', '71 points', '277 points', '51 points', '57 points', '8 points', '42 points', '28 points', '300 points', '102 points', '236 points', '202 points', '91 points', '371 points', '45 points', '113 points', '118 points', '71 points', '146 points', '359 points', '67 points', '133 points', '23 points', '6 points', '32 points', '135 points', '12 points', '77 points', '14 points', '5 points', '51 points', '86 points', '182 points', '234 points', '19 points', '49 points', '122 points', '36 points', '5 points', '9 points', '4 points', '10 points', '826 points', '122 points', '3 points', '14 points', '156 points', '22 points', '196 points', '15 points', '130 points', '119 points', '154 points']\n",
            "['Dividend Cripples Saudi Aramco', 'macOS Nix setup: an alternative to Homebrew (2020)', 'Carlos Ghosn: How I escaped Japan in a box', 'RabbitMQ Streams Overview', 'If you want to transform IT, start with finance', 'Beating TimSort at Merging', 'The worst volume control UI in the world', 'Why is standing more tiring than walking? [audio]', 'Give me /events, not webhooks', 'Plane Food Simulator 2021', 'Reducing the Computational Cost of Deep Reinforcement Learning Research', 'Mozart Data (YC S20) Is Hiring a UI/UX Designer', 'EXIF-based intrinsic image sizing explainer', 'Scalable but Wasteful, or why fast replication protocols are slow', 'Jelly', '$100M xPrize for Carbon Removal', 'AWS Cost Saving Recommendations', 'Inventor harvests methane gas from ditches and ponds to power his moped', 'MacKichan Software, maker of Scientific Word, has gone out of business', 'Emacs Tramp over AWS SSM APIs', 'Firefox 90', 'Night of the Guillotine: The Fall of Robespierre', 'A privacy war is raging inside the W3C', 'Show HN: maildog - Hosting email forwarding service on AWS with GitHub Actions', 'Good and Bad Monitoring', 'Apache Heron: A realtime, distributed, fault-tolerant stream processing engine', 'Building a vision of life without work (2015)', 'Launch HN: Café (YC S21) – Find the best days to go to the office', \"Downgrade User Agent Client Hints to 'harmful'\", 'A review and how-to guide for Microsoft Form Recognizer', 'How to Squeeze a Lexicon (2002) [pdf]', 'Proust’s Madeleine Was Originally a Slice of Toast', 'Ask HN: Introduction to Analog Synthesizers (Simulation OK)', 'It’s official. Your private communications can (and will) be spied on', 'Go 1.17 Release Candidate 1', 'Amazon vanishes 1984 from Kindles (2009)', 'App Tracking Transparency causing 15% to 20% revenue drop for advertisers', 'Comic Books and Juvenile Delinquency (1955)', 'The History of Standard ML [audio]', 'Burden of post-Covid-19 syndrome and implications for healthcare planning', 'Lachlan Morton completes 5,510km Alt Tour, beating peloton to Paris by five days', 'A Chemical Hunger – Part III: Environmental Contaminants', 'To compute a constant of calculus (2019)', 'A commemoration of Edsger Dijkstra [pdf]', \"Schopenhauer's 38 Stratagems, or 38 Ways to Win an Argument\", 'Google fined €500m by French competition authority', 'JupyterLite: Jupyter WebAssembly Python', \"The State of Python Packaging in 2021 – Bastian Venthur's Blog\", 'Apple just launched an official $99 MagSafe battery pack for iPhone 12 lineup', 'This Website is hosted on an Casio fx-9750GII Calculator', 'Plenty of exercise can be enough to offset the negative impacts of poor sleep', 'The brain needs animal fat (2019)', 'Email Forwarding Is Bad: Why you should never forward email', 'Apple MagSafe Battery Pack', 'Seven Centuries Too Late', 'Do nothing (2020)', 'The Count of Monte Cristo is inspired by a real framing-revenge plot story', 'Show HN: Fakeflix – Netflix open source clone', 'Elm Compiler Written in Elm', 'What modern animals would look like if we drew them in the same way as dinosaurs']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSxHvY5bzfP7",
        "outputId": "c8e41951-0bb9-4757-c412-fcf913760ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>post_age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Npm audit: Broken by Design</td>\n",
              "      <td>2 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coming to Terms with Tailwind</td>\n",
              "      <td>1 hour ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Neurons Unexpectedly Encode Information in the...</td>\n",
              "      <td>1 hour ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ideas in statistics that have powered AI</td>\n",
              "      <td>3 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Show HN: SQLite query inside a Bash function</td>\n",
              "      <td>1 hour ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>Gombe Chimpanzee War</td>\n",
              "      <td>13 hours ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>Microbes in cow stomachs may help our plastic ...</td>\n",
              "      <td>2 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>Measuring memory usage in Python: it’s tricky</td>\n",
              "      <td>2 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>The Chelsea Hotel Becomes a New York Battleground</td>\n",
              "      <td>2 days ago</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>IEA sees gas rebound and issues climate warning</td>\n",
              "      <td>20 hours ago</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>210 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 title      post_age\n",
              "0                          Npm audit: Broken by Design   2 hours ago\n",
              "1                        Coming to Terms with Tailwind    1 hour ago\n",
              "2    Neurons Unexpectedly Encode Information in the...    1 hour ago\n",
              "3             Ideas in statistics that have powered AI   3 hours ago\n",
              "4         Show HN: SQLite query inside a Bash function    1 hour ago\n",
              "..                                                 ...           ...\n",
              "205                               Gombe Chimpanzee War  13 hours ago\n",
              "206  Microbes in cow stomachs may help our plastic ...    2 days ago\n",
              "207      Measuring memory usage in Python: it’s tricky    2 days ago\n",
              "208  The Chelsea Hotel Becomes a New York Battleground    2 days ago\n",
              "209    IEA sees gas rebound and issues climate warning  20 hours ago\n",
              "\n",
              "[210 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0C7d4bSuhVM",
        "outputId": "368ee093-0182-4a15-8d18-7341ac1832fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.title[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Npm audit: Broken by Design'},\n",
              " {'title': 'Coming to Terms with Tailwind'},\n",
              " {'title': 'Project Oberon Emulator in JavaScript and Java'},\n",
              " {'title': 'Ideas in statistics that have powered AI'},\n",
              " {'title': 'Haiti’s President Is Assassinated'},\n",
              " {'title': 'Neurons Unexpectedly Encode Information in the Timing of Their Firing'},\n",
              " {'title': \"Apple's “iCloud Private Relay” broke risk based authentication\"},\n",
              " {'title': 'Complex game worlds, simple text interfaces (2015)'},\n",
              " {'title': 'Open letter: Ban surveillance-based advertising'},\n",
              " {'title': 'Show HN: SQLite query inside a Bash function'},\n",
              " {'title': 'Libimobiledevice – Open-source library to communicate with iOS devices natively'},\n",
              " {'title': 'Apollo 11 implementation of Trigonometric functions (1969)'},\n",
              " {'title': 'Reddit.com weighs 1.39MB. Here’s what that costs around the globe'},\n",
              " {'title': 'Bonsai (YC W16) Is Hiring a Head of Product (Remote)'},\n",
              " {'title': \"AI bot trolls politicians with how much time they're looking at phones\"},\n",
              " {'title': 'Game Gulf (2013)'},\n",
              " {'title': 'Ketchum, Idaho, Has Plenty of Available Jobs, but Workers Can’t Afford Housing'},\n",
              " {'title': 'First new VAX in 30 years?'},\n",
              " {'title': 'A classic Silicon Valley tactic – losing money to crush rivals – under scrutiny'},\n",
              " {'title': 'Microsoft Issues Emergency Patch for Windows Flaw'},\n",
              " {'title': 'Designing Our Serverless Engine: From Kubernetes to Nomad, Firecracker, and Kuma'},\n",
              " {'title': 'Social Media Platforms as Common Carriers [pdf]'},\n",
              " {'title': 'JanusGraph – Distributed, open source, scalable graph database'},\n",
              " {'title': 'macOS extensions are moving away from the kernel'},\n",
              " {'title': 'Dbcritic: Constructively criticizing your Postgres schema (using Idris)'},\n",
              " {'title': 'Corona Game'},\n",
              " {'title': 'Right to repair movement gains power in US and Europe'},\n",
              " {'title': 'SQL Join Types Explained in Visuals'},\n",
              " {'title': 'Towards a more Elvish vision for Technology'},\n",
              " {'title': 'Reconcile All the Things'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMRV5RJkrLJa"
      },
      "source": [
        "df = collect_information(2)\n",
        "assert df.shape == (60, 3)\n",
        "assert df.columns[0] == \"title\"\n",
        "\n",
        "df = collect_information(5)\n",
        "assert df.shape == (150, 3)\n",
        "assert df.columns[2] == \"age\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvix8WGerLJa"
      },
      "source": [
        "## Exercise\n",
        "This lesson for the sub-project you will need to scrape reddit's homepage. As it can be difficult to do it with the new Reddit's design, you can visit the old one instead - [old.reddit.com](https://old.reddit.com). You will need to complete these tasks:\n",
        "1. Visit old.reddit.com and look at its layout\n",
        "2. Create a function that can scrape pages of Reddit. It should return a dataset with: `post score`, `post title`, `post thumb URL`, `posts comments count`, `posts subreddit`. All the missing information should be replaced with `None` values\n",
        "3. Scrape website and create dataframe that has at least 300 rows\n",
        "4. Export dataframe's data to csv format and save it in your repository as `reddit_data.csv`\n",
        "\n",
        "**IMPORTANT**: You might want to check out [this](https://pypi.org/project/fake-useragent/) Python package that creates a fake user agent for the request.\n",
        "\n",
        "Do not forget to write clean code!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U07GoQ5yrLJa"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrY3vCnxrLJa"
      },
      "source": [
        "## Summary\n",
        "If you want to create your own dataset, collecting it by using web scraping technique is one of the ways of doing it. Sometimes data will not be presented in pretty preprocessed csv columns and you will need to extract it from various resources by yourself. Automation of mentioned process can sometimes be a key part of a successful ML Project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3DWsJSPrLJa"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJKLYICZrLJb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}